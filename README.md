# Neulix DataHub ğŸ•¸ï¸

**Neulix DataHub** Ã© uma plataforma modular para automaÃ§Ã£o, coleta e visualizaÃ§Ã£o de dados, totalmente orquestrada via Airflow e Docker. Permite criar crawlers genÃ©ricos (scrapers e automaÃ§Ãµes) facilmente extensÃ­veis para qualquer site, com integraÃ§Ã£o direta ao PostgreSQL e dashboards em tempo real via Streamlit.

---

### ğŸ“Œ Principais Funcionalidades

- Pipelines de scraping e automaÃ§Ã£o totalmente automatizadas (ex: LinkedIn, Wikipedia)
- Arquitetura modular e extensÃ­vel (spiders, transformers, loaders)
- Dashboards em tempo real (Streamlit)
- Interface web pÃºblica (Nginx reverse proxy)
- Pronto para rodar em Raspberry Pi ou servidores
- HTTPS via DDNS e Letâ€™s Encrypt

---

### ğŸ§± Stack TecnolÃ³gica

| Camada         | Ferramenta                       |
|---------------|-----------------------------------|
| OrquestraÃ§Ã£o  | Apache Airflow                    |
| Scraping      | Selenium (Firefox), BeautifulSoup |
| ETL           | pandas, SQLAlchemy                |
| Storage       | PostgreSQL                        |
| VisualizaÃ§Ã£o  | Streamlit                         |
| Proxy         | Nginx + Letâ€™s Encrypt             |
| Container     | Docker + docker-compose           |

---

### ğŸ—‚ Estrutura de DiretÃ³rios

```bash
neulix_datahub/
â”œâ”€â”€ core/             # DAGs do Airflow
â”œâ”€â”€ neulix_dataflow/  # Spiders, Transformers, Loaders
â”‚   â””â”€â”€ spiders/      # Spiders genÃ©ricos (Selenium)
â”œâ”€â”€ neulix_interface/ # Dashboards Streamlit
â”œâ”€â”€ nginx/            # ConfiguraÃ§Ã£o do Nginx
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example      # VariÃ¡veis de ambiente
```

---

### ğŸš€ Como Rodar

```bash
git clone https://github.com/seu-usuario/neulix_datahub.git
cd neulix_datahub
cp .env.example .env
docker-compose up --build -d
```

Acesse:
- http://localhost/airflow â€“ Gerenciamento de DAGs
- http://localhost/dashboard â€“ Dashboards Streamlit

---

### ğŸ•¸ï¸ Exemplo de Spiders

* 

Todos os spiders herdam de `BaseSpider`, que utiliza Selenium (Firefox) para automaÃ§Ã£o e scraping. Novos spiders podem ser criados facilmente herdando essa base.

---

### ğŸ”„ OrquestraÃ§Ã£o

- Cada spider sao executados por um DAG do Airflow (exemplos em `core/`).
- Resultados sao salvos no PostgreSQL e consumidos pelo Streamlit.

---

### ğŸ‘¨â€ğŸ’» Notas de Dev

- Spiders ficam em `neulix_dataflow/spiders/`, cada um pode ser disparado por um DAG.
- Dashboards sÃ£o construÃ­dos sob demanda a partir do banco PostgreSQL.
- Arquitetura modular: fÃ¡cil adicionar novos spiders, transformadores e loaders.
